{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = r'C:\\Users\\Yang\\Desktop\\project'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yang\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models import FastText\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "import csv\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (1) Import model (단어를 벡터로 바꾸는 아래의 3가지 모델 중 하나 선택)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1-1) Word2vec model import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yang\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\smart_open\\smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    }
   ],
   "source": [
    "model = Word2Vec.load(base_dir + r'\\word_model_70000\\Word2vec.model')\n",
    "word_vectors = model.wv\n",
    "vocabs = word_vectors.vocab.keys()\n",
    "word_vectors_list = [word_vectors[v] for v in vocabs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1-2) Fasttext model import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yang\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\smart_open\\smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    }
   ],
   "source": [
    "model = FastText.load(base_dir + r'\\word_model\\Fasttext.model')\n",
    "word_vectors = model.wv\n",
    "vocabs = word_vectors.vocab.keys()\n",
    "word_vectors_list = [word_vectors[v] for v in vocabs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1-3) Doc2vec model import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Doc2Vec.load(base_dir + r'\\word_model\\Doc2vec.model')\n",
    "word_vectors = model.wv\n",
    "vocabs = word_vectors.vocab.keys()\n",
    "word_vectors_list = [word_vectors[v] for v in vocabs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1-4) Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "for token_sent in token_sents:\n",
    "    sent = ''\n",
    "    for i in range(len(token_sent)):\n",
    "        voca = token_sent[i][:token_sent[i].index('/')]\n",
    "        if i is not len(token_sent)-1:\n",
    "            sent = sent+voca+' '\n",
    "        else:\n",
    "            sent = sent+voca\n",
    "    corpus.append(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vect = CountVectorizer()\n",
    "vect.fit(corpus)\n",
    "vec_sents = vect.transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) Train data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_utility import read_damage_csv\n",
    "sents, label_damage = read_damage_csv(base_dir + r'\\data\\data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 품사 태깅(주어진 텍스트를 형태소 단위로 나누고 나눠진 형태소를 해당하는 품사와 함께 리스트화)\n",
    "# stemming(단어들을 원형으로 포현), normalization(표현 방법이 다른 단어들을 통합시켜 같은 단어로 만듬)\n",
    "import json\n",
    "import os\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "okt = Okt()\n",
    "\n",
    "def tokenize(doc): # 형태소 분석기인 Okt를 이용해서 품사를 태깅시킴\n",
    "    # norm은 정규화, stem은 근어로 표시하기를 나타냄\n",
    "    return ['/'.join(t) for t in okt.pos(doc, norm=True, stem=True)]\n",
    "\n",
    "os.chdir(base_dir) # 경로 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_sents = [] # word_vectors의 key가 '단어/품사'의 형식으로 되어 있기 때문에, 같은 형식으로 만들기 위해 각 문장별로 tokenize를 적용함.\n",
    "for sent in sents:\n",
    "    token_sents.append(tokenize(sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3) Vectorization(word2vec, fasttest, doc2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_utility import token2vec\n",
    "vec_sents, label_damage = token2vec(token_sents, label_damage, word_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델링 - Machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) 문장 속 단어들의 Vector(word2vec, fasttext, doc2vec)의 평균값을 이용해서 문장 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Embedding 일 때\n",
    "import numpy as np\n",
    "\n",
    "sentences = np.asarray(vec_sents)\n",
    "mean_sents = []\n",
    "for i in range(len(sentences)):\n",
    "    #mean_sents = mean_sents + np.mean(sentences[i], axis=0)\n",
    "    mean_sents.append(np.mean(sentences[i], axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## bag of words 일 때\n",
    "# (bag of words가 아닐 때는 실행하면 안됨!!)\n",
    "mean_sents = vec_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3789924267369114"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.array(label_damage) == '1')/len(label_damage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1-1) Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yang\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Yang\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Yang\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Yang\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Yang\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross-val-score \n",
      "[0.63486842 0.85361842 0.8583196  0.86820428 0.74958814]\n",
      "cross-val-score.mean \n",
      "0.793\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "scores = cross_val_score(logreg, mean_sents, label_damage, cv=5) \n",
    "print('cross-val-score \\n{}'.format(scores))\n",
    "print('cross-val-score.mean \\n{:.3f}'.format(scores.mean()))\n",
    "# 전체에서 0인 것 57%, 1인 것 43%인데 결과가 84%면 괜찮은 듯.. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1-2) SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross-val-score \n",
      "[0.60690789 0.82565789 0.81878089 0.82701812 0.71828666]\n",
      "cross-val-score.mean \n",
      "0.759\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "svm = SVC(kernel='linear', C=1.0, random_state=0, gamma=0.10)\n",
    "scores_svm = cross_val_score(svm, mean_sents, label_damage, cv=5) \n",
    "print('cross-val-score \\n{}'.format(scores_svm))\n",
    "print('cross-val-score.mean \\n{:.3f}'.format(scores_svm.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1-3) Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross-val-score \n",
      "[0.62828947 0.63404605 0.62602965 0.62355848 0.62438221]\n",
      "cross-val-score.mean \n",
      "0.627\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=0)\n",
    "scores_rf = cross_val_score(rf, mean_sents, label_damage, cv=5) \n",
    "print('cross-val-score \\n{}'.format(scores_rf))\n",
    "print('cross-val-score.mean \\n{:.3f}'.format(scores_rf.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1-4) XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xgboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-a4b4b7f24e94>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mxgboost\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mxgb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mlabel_damage2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel_damage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdata_dmatrix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDMatrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmean_sents\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlabel_damage2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mparams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m\"objective\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m\"reg:linear\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'colsample_bytree'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m0.3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'learning_rate'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'max_depth'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'alpha'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m cv_results = xgb.cv(dtrain=data_dmatrix, params=params, nfold=3,\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'xgboost'"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "label_damage2 = list(map(int, label_damage))\n",
    "data_dmatrix = xgb.DMatrix(data=mean_sents,label=label_damage2)\n",
    "params = {\"objective\":\"reg:linear\",'colsample_bytree': 0.3,'learning_rate': 0.1,'max_depth': 5, 'alpha': 10}\n",
    "cv_results = xgb.cv(dtrain=data_dmatrix, params=params, nfold=3,\n",
    "                    num_boost_round=50,early_stopping_rounds=10,metrics=\"auc\", as_pandas=True, seed=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((cv_results[\"test-auc-mean\"]).tail(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_xgb, X_test_xgb, y_train_xgb, y_test_xgb = train_test_split(mean_sents, label_damage2, test_size=0.2, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_reg = xgb.XGBRegressor(objective ='reg:linear', colsample_bytree = 0.3, learning_rate = 0.1,\n",
    "                max_depth = 5, alpha = 10, n_estimators = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_reg.fit(X_train_xgb,y_train_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "preds = xg_reg.predict(X_test_xgb)\n",
    "preds_cat = [1 if x > 0.5 else 0 for x in preds]\n",
    "sum(np.array(y_test_xgb) ==  np.array(preds_cat))/len(y_test_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump(xg_reg, open(\"xgb.dat\", \"wb\")) # 모델 저장"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델링 - Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding 된 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import LSTM\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Bidirectional\n",
    "from keras.preprocessing import sequence\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "sentences = np.asarray(vec_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_utility import zero_padding, circular_padding, reverse_padding\n",
    "new_sentences = zero_padding(sentences)\n",
    "#new_sentences = circular_padding(sentences)\n",
    "#new_sentences = reverse_padding(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = new_sentences\n",
    "y = np_utils.to_categorical(np.asarray(label_damage))\n",
    "x_train, x_test, y_train, y_test = train_test_split(new_sentences,y,test_size=0.2,random_state=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) LSTM을 이용해서 문장 분류"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2-1) Keras 내부 Embedding layer 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어에 대해 숫자로 매핑시킴.\n",
    "def word2num(list_2d):\n",
    "    w2n_dic = dict()  # word가 key이고 index가 value인 dict\n",
    "    n2w_dic = dict()  # index가 key이고 word가 value인 dict. 나중에 번호에서 단어로 쉽게 바꾸기 위해.\n",
    "    idx = 1\n",
    "    num_list = [[] for _ in range(len(list_2d))]   # 숫자에 매핑된 글의 리스트\n",
    "    for k,i in enumerate(list_2d):\n",
    "        if not i:\n",
    "            continue\n",
    "        elif isinstance(i, str): \n",
    "             # 내용이 단어 하나로 이루어진 경우, for loop으로 ['단어']가 '단'과 '어'로 나뉘지 않게 한다.\n",
    "            if w2n_dic.get(i) is None:\n",
    "                w2n_dic[i] = idx\n",
    "                n2w_dic[idx] = i\n",
    "                idx += 1\n",
    "            num_list[k] = [dic[i]]\n",
    "        else:\n",
    "            for j in i:\n",
    "                if w2n_dic.get(j) is None:\n",
    "                    w2n_dic[j] = idx\n",
    "                    n2w_dic[idx] = j\n",
    "                    idx += 1\n",
    "                num_list[k].append(w2n_dic[j])\n",
    "    return num_list, w2n_dic, n2w_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_list, w2n_dic, n2w_dic = word2num(token_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = sequence.pad_sequences(num_list, maxlen=50)\n",
    "y = np_utils.to_categorical(np.asarray(label_damage))\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2,random_state=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(len(w2n_dic)+1,100))\n",
    "model.add(LSTM(100, dropout=0.5, recurrent_dropout=0.5, activation='tanh'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x_train, y_train, batch_size=100, epochs=100, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2-2) 이미 Embedding된 데이터 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2, activation='tanh'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4859 samples, validate on 1215 samples\n",
      "Epoch 1/25\n",
      "4859/4859 [==============================] - 28s 6ms/step - loss: 0.5140 - acc: 0.7590 - val_loss: 0.4297 - val_acc: 0.8272\n",
      "Epoch 2/25\n",
      "4859/4859 [==============================] - 13s 3ms/step - loss: 0.4347 - acc: 0.8197 - val_loss: 0.4388 - val_acc: 0.8255\n",
      "Epoch 3/25\n",
      "4859/4859 [==============================] - 14s 3ms/step - loss: 0.4213 - acc: 0.8265 - val_loss: 0.3896 - val_acc: 0.8387\n",
      "Epoch 4/25\n",
      "4859/4859 [==============================] - 13s 3ms/step - loss: 0.3927 - acc: 0.8312 - val_loss: 0.3560 - val_acc: 0.8387\n",
      "Epoch 5/25\n",
      "4859/4859 [==============================] - 20s 4ms/step - loss: 0.3910 - acc: 0.8317 - val_loss: 0.3675 - val_acc: 0.8412\n",
      "Epoch 6/25\n",
      "4859/4859 [==============================] - 13s 3ms/step - loss: 0.3878 - acc: 0.8368 - val_loss: 0.3623 - val_acc: 0.8321\n",
      "Epoch 7/25\n",
      "4859/4859 [==============================] - 13s 3ms/step - loss: 0.3734 - acc: 0.8450 - val_loss: 0.3800 - val_acc: 0.8420\n",
      "Epoch 8/25\n",
      "4859/4859 [==============================] - 13s 3ms/step - loss: 0.3682 - acc: 0.8454 - val_loss: 0.3680 - val_acc: 0.8296\n",
      "Epoch 9/25\n",
      "4859/4859 [==============================] - 13s 3ms/step - loss: 0.3581 - acc: 0.8432 - val_loss: 0.4162 - val_acc: 0.8477\n",
      "Epoch 10/25\n",
      "4859/4859 [==============================] - 13s 3ms/step - loss: 0.3608 - acc: 0.8531 - val_loss: 0.3807 - val_acc: 0.8551\n",
      "Epoch 11/25\n",
      "4859/4859 [==============================] - 13s 3ms/step - loss: 0.3513 - acc: 0.8545 - val_loss: 0.3761 - val_acc: 0.8494\n",
      "Epoch 12/25\n",
      "4859/4859 [==============================] - 13s 3ms/step - loss: 0.3476 - acc: 0.8570 - val_loss: 0.3427 - val_acc: 0.8609\n",
      "Epoch 13/25\n",
      "4859/4859 [==============================] - 14s 3ms/step - loss: 0.3350 - acc: 0.8631 - val_loss: 0.3362 - val_acc: 0.8551\n",
      "Epoch 14/25\n",
      "4859/4859 [==============================] - 14s 3ms/step - loss: 0.3285 - acc: 0.8609 - val_loss: 0.3209 - val_acc: 0.8634\n",
      "Epoch 15/25\n",
      "4859/4859 [==============================] - 14s 3ms/step - loss: 0.3202 - acc: 0.8685 - val_loss: 0.3391 - val_acc: 0.8667\n",
      "Epoch 16/25\n",
      "4859/4859 [==============================] - 14s 3ms/step - loss: 0.3244 - acc: 0.8664 - val_loss: 0.3346 - val_acc: 0.8543\n",
      "Epoch 17/25\n",
      "4859/4859 [==============================] - 14s 3ms/step - loss: 0.3148 - acc: 0.8660 - val_loss: 0.3160 - val_acc: 0.8716\n",
      "Epoch 18/25\n",
      "4859/4859 [==============================] - 14s 3ms/step - loss: 0.3190 - acc: 0.8708 - val_loss: 0.3138 - val_acc: 0.8626\n",
      "Epoch 19/25\n",
      "4859/4859 [==============================] - 14s 3ms/step - loss: 0.3087 - acc: 0.8689 - val_loss: 0.3436 - val_acc: 0.8551\n",
      "Epoch 20/25\n",
      "4859/4859 [==============================] - 14s 3ms/step - loss: 0.3038 - acc: 0.8710 - val_loss: 0.3248 - val_acc: 0.8675\n",
      "Epoch 21/25\n",
      "4859/4859 [==============================] - 14s 3ms/step - loss: 0.2956 - acc: 0.8794 - val_loss: 0.3208 - val_acc: 0.8634\n",
      "Epoch 22/25\n",
      "4859/4859 [==============================] - 14s 3ms/step - loss: 0.3020 - acc: 0.8759 - val_loss: 0.3352 - val_acc: 0.8642\n",
      "Epoch 23/25\n",
      "4859/4859 [==============================] - 21s 4ms/step - loss: 0.2872 - acc: 0.8794 - val_loss: 0.3328 - val_acc: 0.8551\n",
      "Epoch 24/25\n",
      "4859/4859 [==============================] - 15s 3ms/step - loss: 0.2792 - acc: 0.8829 - val_loss: 0.3314 - val_acc: 0.8749\n",
      "Epoch 25/25\n",
      "4859/4859 [==============================] - 14s 3ms/step - loss: 0.2814 - acc: 0.8841 - val_loss: 0.3352 - val_acc: 0.8749\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train, batch_size=100, epochs=25, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('\\model\\model_doc2vec_reversePadding.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3) CNN을 이용해서 문장 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(layers.Conv1D(filters = 50, kernel_size = 10, activation='relu'))\n",
    "model.add(layers.Dropout(0.2))\n",
    "model.add(layers.Conv1D(filters = 30, kernel_size = 5, activation='relu'))\n",
    "model.add(layers.Conv1D(filters = 20, kernel_size = 3, activation='relu'))\n",
    "#model.add(layers.LSTM(15))\n",
    "model.add(layers.GlobalMaxPool1D())\n",
    "model.add(layers.Dropout(0.2))\n",
    "model.add(layers.Dense(10, activation='relu'))\n",
    "model.add(layers.Dropout(0.2))\n",
    "model.add(layers.Dense(2, activation='softmax'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (None, 50, 100)\n",
    "model.build(input_shape)\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x_train, y_train, batch_size=100, epochs=30, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (4) BRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(patience = 5) # 조기종료 콜백함수 정의\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(100, dropout=0.2, recurrent_dropout=0.2, activation='tanh')))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4858 samples, validate on 1215 samples\n",
      "Epoch 1/25\n",
      "4858/4858 [==============================] - 36s 7ms/step - loss: 0.4749 - acc: 0.7717 - val_loss: 0.3759 - val_acc: 0.8346\n",
      "Epoch 2/25\n",
      "4858/4858 [==============================] - 23s 5ms/step - loss: 0.3742 - acc: 0.8351 - val_loss: 0.3597 - val_acc: 0.8461\n",
      "Epoch 3/25\n",
      "4858/4858 [==============================] - 23s 5ms/step - loss: 0.3282 - acc: 0.8600 - val_loss: 0.3586 - val_acc: 0.8469\n",
      "Epoch 4/25\n",
      "4858/4858 [==============================] - 23s 5ms/step - loss: 0.2976 - acc: 0.8730 - val_loss: 0.3451 - val_acc: 0.8551\n",
      "Epoch 5/25\n",
      "4858/4858 [==============================] - 23s 5ms/step - loss: 0.2707 - acc: 0.8872 - val_loss: 0.3419 - val_acc: 0.8576\n",
      "Epoch 6/25\n",
      "4858/4858 [==============================] - 22s 5ms/step - loss: 0.2353 - acc: 0.9065 - val_loss: 0.3711 - val_acc: 0.8568\n",
      "Epoch 7/25\n",
      "4858/4858 [==============================] - 22s 5ms/step - loss: 0.2119 - acc: 0.9166 - val_loss: 0.3790 - val_acc: 0.8568\n",
      "Epoch 8/25\n",
      "4858/4858 [==============================] - 22s 5ms/step - loss: 0.1819 - acc: 0.9302 - val_loss: 0.4406 - val_acc: 0.8469\n",
      "Epoch 9/25\n",
      "4858/4858 [==============================] - 28s 6ms/step - loss: 0.1604 - acc: 0.9397 - val_loss: 0.4543 - val_acc: 0.8584\n",
      "Epoch 10/25\n",
      "4858/4858 [==============================] - 26s 5ms/step - loss: 0.1421 - acc: 0.9481 - val_loss: 0.4608 - val_acc: 0.8486\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train, batch_size=100, epochs=25, validation_data=(x_test, y_test), callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (5) 앙상블 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 비관련 3천개 데이터 복원 추출해서 앙상블"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4251 samples, validate on 1823 samples\n",
      "Epoch 1/30\n",
      "4251/4251 [==============================] - 21s 5ms/step - loss: 0.6736 - acc: 0.6168 - val_loss: 0.6560 - val_acc: 0.6210\n",
      "Epoch 2/30\n",
      "4251/4251 [==============================] - 8s 2ms/step - loss: 0.6470 - acc: 0.6248 - val_loss: 0.6043 - val_acc: 0.6451\n",
      "Epoch 3/30\n",
      "4251/4251 [==============================] - 7s 2ms/step - loss: 0.5810 - acc: 0.6862 - val_loss: 0.5001 - val_acc: 0.7926\n",
      "Epoch 4/30\n",
      "4251/4251 [==============================] - 8s 2ms/step - loss: 0.5396 - acc: 0.7594 - val_loss: 0.4724 - val_acc: 0.8009\n",
      "Epoch 5/30\n",
      "4251/4251 [==============================] - 8s 2ms/step - loss: 0.5196 - acc: 0.7713 - val_loss: 0.4610 - val_acc: 0.8102\n",
      "Epoch 6/30\n",
      "4251/4251 [==============================] - 8s 2ms/step - loss: 0.5120 - acc: 0.7808 - val_loss: 0.4611 - val_acc: 0.8190\n",
      "Epoch 7/30\n",
      "4251/4251 [==============================] - 8s 2ms/step - loss: 0.5084 - acc: 0.7864 - val_loss: 0.4379 - val_acc: 0.8261\n",
      "Epoch 8/30\n",
      "4251/4251 [==============================] - 8s 2ms/step - loss: 0.4943 - acc: 0.7937 - val_loss: 0.4323 - val_acc: 0.8316\n",
      "Epoch 9/30\n",
      "4251/4251 [==============================] - 8s 2ms/step - loss: 0.4783 - acc: 0.7935 - val_loss: 0.4258 - val_acc: 0.8272\n",
      "Epoch 10/30\n",
      "4251/4251 [==============================] - 8s 2ms/step - loss: 0.4734 - acc: 0.8048 - val_loss: 0.4169 - val_acc: 0.8305\n",
      "Epoch 11/30\n",
      "4251/4251 [==============================] - 8s 2ms/step - loss: 0.4648 - acc: 0.8090 - val_loss: 0.4097 - val_acc: 0.8332\n",
      "Epoch 12/30\n",
      "4251/4251 [==============================] - 8s 2ms/step - loss: 0.4458 - acc: 0.8156 - val_loss: 0.4065 - val_acc: 0.8442\n",
      "Epoch 13/30\n",
      "4251/4251 [==============================] - 8s 2ms/step - loss: 0.4522 - acc: 0.8151 - val_loss: 0.3978 - val_acc: 0.8376\n",
      "Epoch 14/30\n",
      "4251/4251 [==============================] - 8s 2ms/step - loss: 0.4398 - acc: 0.8179 - val_loss: 0.3882 - val_acc: 0.8524\n",
      "Epoch 15/30\n",
      "4251/4251 [==============================] - 8s 2ms/step - loss: 0.4322 - acc: 0.8212 - val_loss: 0.3880 - val_acc: 0.8453\n",
      "Epoch 16/30\n",
      "4251/4251 [==============================] - 8s 2ms/step - loss: 0.4375 - acc: 0.8224 - val_loss: 0.3923 - val_acc: 0.8470\n",
      "Epoch 17/30\n",
      "4251/4251 [==============================] - 8s 2ms/step - loss: 0.4316 - acc: 0.8283 - val_loss: 0.3765 - val_acc: 0.8502\n",
      "Epoch 18/30\n",
      "4251/4251 [==============================] - 8s 2ms/step - loss: 0.4185 - acc: 0.8283 - val_loss: 0.3716 - val_acc: 0.8524\n",
      "Epoch 19/30\n",
      "4251/4251 [==============================] - 8s 2ms/step - loss: 0.4144 - acc: 0.8346 - val_loss: 0.3678 - val_acc: 0.8524\n",
      "Epoch 20/30\n",
      "4251/4251 [==============================] - 8s 2ms/step - loss: 0.4086 - acc: 0.8285 - val_loss: 0.3573 - val_acc: 0.8552\n",
      "Epoch 21/30\n",
      "4251/4251 [==============================] - 9s 2ms/step - loss: 0.4006 - acc: 0.8363 - val_loss: 0.3579 - val_acc: 0.8491\n",
      "Epoch 22/30\n",
      "4251/4251 [==============================] - 10s 2ms/step - loss: 0.4036 - acc: 0.8285 - val_loss: 0.3612 - val_acc: 0.8557\n",
      "Epoch 23/30\n",
      "4251/4251 [==============================] - 10s 2ms/step - loss: 0.4028 - acc: 0.8358 - val_loss: 0.3542 - val_acc: 0.8502\n",
      "Epoch 24/30\n",
      "4251/4251 [==============================] - 8s 2ms/step - loss: 0.4056 - acc: 0.8295 - val_loss: 0.3501 - val_acc: 0.8568\n",
      "Epoch 25/30\n",
      "4251/4251 [==============================] - 18s 4ms/step - loss: 0.3928 - acc: 0.8457 - val_loss: 0.3509 - val_acc: 0.8552\n",
      "Epoch 26/30\n",
      "4251/4251 [==============================] - 9s 2ms/step - loss: 0.3926 - acc: 0.8358 - val_loss: 0.3496 - val_acc: 0.8524\n",
      "Epoch 27/30\n",
      "4251/4251 [==============================] - 8s 2ms/step - loss: 0.3917 - acc: 0.8335 - val_loss: 0.3425 - val_acc: 0.8585\n",
      "Epoch 28/30\n",
      "4251/4251 [==============================] - 8s 2ms/step - loss: 0.3885 - acc: 0.8372 - val_loss: 0.3422 - val_acc: 0.8612\n",
      "Epoch 29/30\n",
      "4251/4251 [==============================] - 8s 2ms/step - loss: 0.3760 - acc: 0.8473 - val_loss: 0.3404 - val_acc: 0.8568\n",
      "Epoch 30/30\n",
      "4251/4251 [==============================] - 8s 2ms/step - loss: 0.3807 - acc: 0.8464 - val_loss: 0.3388 - val_acc: 0.8601\n",
      "Train on 4251 samples, validate on 1823 samples\n",
      "Epoch 1/30\n",
      "4251/4251 [==============================] - 24s 6ms/step - loss: 0.6627 - acc: 0.6170 - val_loss: 0.6313 - val_acc: 0.6259\n",
      "Epoch 2/30\n",
      "4251/4251 [==============================] - 8s 2ms/step - loss: 0.6041 - acc: 0.6629 - val_loss: 0.5178 - val_acc: 0.7318\n",
      "Epoch 3/30\n",
      "4251/4251 [==============================] - 8s 2ms/step - loss: 0.5464 - acc: 0.7332 - val_loss: 0.4658 - val_acc: 0.8173\n",
      "Epoch 4/30\n",
      "4251/4251 [==============================] - 8s 2ms/step - loss: 0.5136 - acc: 0.7629 - val_loss: 0.4476 - val_acc: 0.8245\n",
      "Epoch 5/30\n",
      "4251/4251 [==============================] - 8s 2ms/step - loss: 0.4959 - acc: 0.7819 - val_loss: 0.4227 - val_acc: 0.8327\n",
      "Epoch 6/30\n",
      "4251/4251 [==============================] - 8s 2ms/step - loss: 0.4968 - acc: 0.7892 - val_loss: 0.4216 - val_acc: 0.8371\n",
      "Epoch 7/30\n",
      "4251/4251 [==============================] - 8s 2ms/step - loss: 0.4723 - acc: 0.8095 - val_loss: 0.4097 - val_acc: 0.8415\n",
      "Epoch 8/30\n",
      "4251/4251 [==============================] - 8s 2ms/step - loss: 0.4838 - acc: 0.8012 - val_loss: 0.4055 - val_acc: 0.8404\n",
      "Epoch 9/30\n",
      "4251/4251 [==============================] - 8s 2ms/step - loss: 0.4706 - acc: 0.8057 - val_loss: 0.4038 - val_acc: 0.8420\n",
      "Epoch 10/30\n",
      "4251/4251 [==============================] - 8s 2ms/step - loss: 0.4570 - acc: 0.8177 - val_loss: 0.4088 - val_acc: 0.8360\n",
      "Epoch 11/30\n",
      "4251/4251 [==============================] - 8s 2ms/step - loss: 0.4584 - acc: 0.8217 - val_loss: 0.3952 - val_acc: 0.8481\n",
      "Epoch 12/30\n",
      "4251/4251 [==============================] - 8s 2ms/step - loss: 0.4445 - acc: 0.8247 - val_loss: 0.3925 - val_acc: 0.8464\n",
      "Epoch 13/30\n",
      "4251/4251 [==============================] - 8s 2ms/step - loss: 0.4538 - acc: 0.8139 - val_loss: 0.4068 - val_acc: 0.8426\n",
      "Epoch 14/30\n",
      "4251/4251 [==============================] - 8s 2ms/step - loss: 0.4417 - acc: 0.8257 - val_loss: 0.3893 - val_acc: 0.8502\n",
      "Epoch 15/30\n",
      "4251/4251 [==============================] - 8s 2ms/step - loss: 0.4429 - acc: 0.8266 - val_loss: 0.3882 - val_acc: 0.8481\n",
      "Epoch 16/30\n",
      "4251/4251 [==============================] - 8s 2ms/step - loss: 0.4353 - acc: 0.8247 - val_loss: 0.3837 - val_acc: 0.8470\n",
      "Epoch 17/30\n",
      "4251/4251 [==============================] - 8s 2ms/step - loss: 0.4396 - acc: 0.8163 - val_loss: 0.3839 - val_acc: 0.8431\n",
      "Epoch 18/30\n",
      "4251/4251 [==============================] - 8s 2ms/step - loss: 0.4183 - acc: 0.8309 - val_loss: 0.3784 - val_acc: 0.8470\n",
      "Epoch 19/30\n",
      "4251/4251 [==============================] - 8s 2ms/step - loss: 0.4318 - acc: 0.8203 - val_loss: 0.3749 - val_acc: 0.8513\n",
      "Epoch 20/30\n",
      "4251/4251 [==============================] - 8s 2ms/step - loss: 0.4282 - acc: 0.8262 - val_loss: 0.3909 - val_acc: 0.8305\n",
      "Epoch 21/30\n",
      "4251/4251 [==============================] - 8s 2ms/step - loss: 0.4104 - acc: 0.8351 - val_loss: 0.4117 - val_acc: 0.8464\n",
      "Epoch 22/30\n",
      "4251/4251 [==============================] - 8s 2ms/step - loss: 0.4168 - acc: 0.8417 - val_loss: 0.3721 - val_acc: 0.8508\n",
      "Epoch 23/30\n",
      "4251/4251 [==============================] - 8s 2ms/step - loss: 0.4137 - acc: 0.8349 - val_loss: 0.3717 - val_acc: 0.8546\n",
      "Epoch 24/30\n",
      "4251/4251 [==============================] - 8s 2ms/step - loss: 0.4323 - acc: 0.8283 - val_loss: 0.3713 - val_acc: 0.8524\n",
      "Epoch 25/30\n",
      "4251/4251 [==============================] - 8s 2ms/step - loss: 0.4044 - acc: 0.8424 - val_loss: 0.3660 - val_acc: 0.8546\n",
      "Epoch 26/30\n",
      "4251/4251 [==============================] - 8s 2ms/step - loss: 0.4087 - acc: 0.8356 - val_loss: 0.3652 - val_acc: 0.8541\n",
      "Epoch 27/30\n",
      "4251/4251 [==============================] - 8s 2ms/step - loss: 0.4124 - acc: 0.8327 - val_loss: 0.3663 - val_acc: 0.8563\n",
      "Epoch 28/30\n",
      "4251/4251 [==============================] - 14s 3ms/step - loss: 0.4051 - acc: 0.8363 - val_loss: 0.3633 - val_acc: 0.8579\n",
      "Epoch 29/30\n",
      "4251/4251 [==============================] - 8s 2ms/step - loss: 0.3949 - acc: 0.8410 - val_loss: 0.3547 - val_acc: 0.8541\n",
      "Epoch 30/30\n",
      "4251/4251 [==============================] - 8s 2ms/step - loss: 0.3976 - acc: 0.8440 - val_loss: 0.3545 - val_acc: 0.8557\n",
      "Train on 4251 samples, validate on 1823 samples\n",
      "Epoch 1/30\n",
      "4251/4251 [==============================] - 21s 5ms/step - loss: 0.6736 - acc: 0.6140 - val_loss: 0.6540 - val_acc: 0.6221\n",
      "Epoch 2/30\n",
      "4251/4251 [==============================] - 7s 2ms/step - loss: 0.6224 - acc: 0.6382 - val_loss: 0.5369 - val_acc: 0.6939\n",
      "Epoch 3/30\n",
      "4251/4251 [==============================] - 14s 3ms/step - loss: 0.5580 - acc: 0.7179 - val_loss: 0.4874 - val_acc: 0.8003\n",
      "Epoch 4/30\n",
      "4251/4251 [==============================] - 8s 2ms/step - loss: 0.5395 - acc: 0.7396 - val_loss: 0.4710 - val_acc: 0.8184\n",
      "Epoch 5/30\n",
      "4251/4251 [==============================] - 7s 2ms/step - loss: 0.5244 - acc: 0.7506 - val_loss: 0.4588 - val_acc: 0.8140\n",
      "Epoch 6/30\n",
      "4251/4251 [==============================] - 8s 2ms/step - loss: 0.5196 - acc: 0.7619 - val_loss: 0.4500 - val_acc: 0.8294\n",
      "Epoch 7/30\n",
      "4251/4251 [==============================] - 8s 2ms/step - loss: 0.5163 - acc: 0.7765 - val_loss: 0.4477 - val_acc: 0.8228\n",
      "Epoch 8/30\n",
      "4251/4251 [==============================] - 8s 2ms/step - loss: 0.5009 - acc: 0.7930 - val_loss: 0.4590 - val_acc: 0.8261\n",
      "Epoch 9/30\n",
      "4251/4251 [==============================] - 8s 2ms/step - loss: 0.4951 - acc: 0.7960 - val_loss: 0.4312 - val_acc: 0.8349\n",
      "Epoch 10/30\n",
      "4251/4251 [==============================] - 8s 2ms/step - loss: 0.4862 - acc: 0.7958 - val_loss: 0.4245 - val_acc: 0.8349\n",
      "Epoch 11/30\n",
      "4251/4251 [==============================] - 8s 2ms/step - loss: 0.4823 - acc: 0.7986 - val_loss: 0.4209 - val_acc: 0.8343\n",
      "Epoch 12/30\n",
      "4251/4251 [==============================] - 8s 2ms/step - loss: 0.4857 - acc: 0.7956 - val_loss: 0.4225 - val_acc: 0.8327\n",
      "Epoch 13/30\n",
      "4251/4251 [==============================] - 8s 2ms/step - loss: 0.4798 - acc: 0.8031 - val_loss: 0.4266 - val_acc: 0.8300\n",
      "Epoch 14/30\n",
      "4251/4251 [==============================] - 8s 2ms/step - loss: 0.4721 - acc: 0.8048 - val_loss: 0.4129 - val_acc: 0.8371\n",
      "Epoch 15/30\n",
      "4251/4251 [==============================] - 8s 2ms/step - loss: 0.4634 - acc: 0.8156 - val_loss: 0.4143 - val_acc: 0.8404\n",
      "Epoch 16/30\n",
      "4251/4251 [==============================] - 8s 2ms/step - loss: 0.4652 - acc: 0.8172 - val_loss: 0.4318 - val_acc: 0.8393\n",
      "Epoch 17/30\n",
      "4251/4251 [==============================] - 8s 2ms/step - loss: 0.4638 - acc: 0.8137 - val_loss: 0.4092 - val_acc: 0.8409\n",
      "Epoch 18/30\n",
      "4251/4251 [==============================] - 8s 2ms/step - loss: 0.4763 - acc: 0.8109 - val_loss: 0.4267 - val_acc: 0.8365\n",
      "Epoch 19/30\n",
      "4251/4251 [==============================] - 8s 2ms/step - loss: 0.4740 - acc: 0.8118 - val_loss: 0.4141 - val_acc: 0.8420\n",
      "Epoch 20/30\n",
      "4251/4251 [==============================] - 8s 2ms/step - loss: 0.4629 - acc: 0.8153 - val_loss: 0.4406 - val_acc: 0.8338\n",
      "Epoch 21/30\n",
      "3600/4251 [========================>.....] - ETA: 1s - loss: 0.4611 - acc: 0.8189- ETA: 4s - loss: 0."
     ]
    }
   ],
   "source": [
    "# '(2) LSTM을 이용해서 문장 분류' 아래의 import 부분 실행 후 진행해야함.\n",
    "from sentence_utility import read_damage_csv, token2vec, zero_padding, circular_padding, reverse_padding\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stopping = EarlyStopping(patience = 10) # 조기종료 콜백함수 정의\n",
    "total_num_model = 20 # 만들 모델 갯수\n",
    "\n",
    "for i in range(total_num_model):\n",
    "    sents, label_damage = read_damage_csv(base_dir + r'\\ensemble_data\\data'+str(i)+r'.csv')\n",
    "    \n",
    "    token_sents = [] # word_vectors의 key가 '단어/품사'의 형식으로 되어 있기 때문에, 같은 형식으로 만들기 위해 각 문장별로 tokenize를 적용함.\n",
    "    for sent in sents:\n",
    "        token_sents.append(tokenize(sent))\n",
    "    \n",
    "    vec_sents, label_damage = token2vec(token_sents, label_damage, word_vectors)\n",
    "        \n",
    "    sentences = np.asarray(vec_sents)\n",
    "    new_sentences = zero_padding(sentences)\n",
    "    #new_sentences = circular_padding(sentences)\n",
    "    #new_sentences = reverse_padding(sentences)\n",
    "    \n",
    "    x = new_sentences\n",
    "    y = np_utils.to_categorical(np.asarray(label_damage))\n",
    "    x_train, x_test, y_train, y_test = train_test_split(new_sentences,y,test_size=0.3,random_state=1, shuffle=True)\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(LSTM(10, dropout=0.2, recurrent_dropout=0.2, activation='tanh'))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='rmsprop',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    history = model.fit(x_train, y_train, batch_size=100, epochs=30, validation_data=(x_test, y_test), callbacks=[early_stopping])\n",
    "    model.save(base_dir+r'\\model_ensemble_20\\lstm_zeroPadding'+str(i)+'.h5')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: 0\n",
      "Model: 1\n",
      "Model: 2\n",
      "Model: 3\n",
      "Model: 4\n",
      "Model: 5\n",
      "Model: 6\n",
      "Model: 7\n",
      "Model: 8\n",
      "Model: 9\n",
      "Model: 10\n",
      "Model: 11\n",
      "Model: 12\n",
      "Model: 13\n",
      "Model: 14\n",
      "Model: 15\n",
      "Model: 16\n",
      "Model: 17\n",
      "Model: 18\n",
      "Model: 19\n",
      "Model: 20\n",
      "Model: 21\n",
      "Model: 22\n",
      "Model: 23\n",
      "Model: 24\n",
      "Model: 25\n",
      "Model: 26\n",
      "Model: 27\n",
      "Model: 28\n",
      "Model: 29\n",
      "Model: 30\n",
      "Model: 31\n",
      "Model: 32\n",
      "Model: 33\n",
      "Model: 34\n",
      "Model: 35\n",
      "Model: 36\n",
      "Model: 37\n",
      "Model: 38\n",
      "Model: 39\n"
     ]
    }
   ],
   "source": [
    "# 만들어진 앙상블 모델로 테스트 진행\n",
    "from keras.models import load_model\n",
    "\n",
    "model_list = [] # 생성한 모델들을 저장\n",
    "total_num_model = 40 # 생성한 모델 갯수\n",
    "\n",
    "for i in range(total_num_model):\n",
    "    print(\"Model: \"+str(i))\n",
    "    model = load_model(base_dir+'\\model_ensemble_40\\lstm_zeroPadding'+str(i)+'.h5') # 위에서 생성한 모델 import\n",
    "    model_list.append(model)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: 0\n",
      "Model: 1\n",
      "Model: 2\n",
      "Model: 3\n",
      "Model: 4\n",
      "Model: 5\n",
      "Model: 6\n",
      "Model: 7\n",
      "Model: 8\n",
      "Model: 9\n",
      "Model: 10\n",
      "Model: 11\n",
      "Model: 12\n",
      "Model: 13\n",
      "Model: 14\n",
      "Model: 15\n",
      "Model: 16\n",
      "Model: 17\n",
      "Model: 18\n",
      "Model: 19\n",
      "Model: 20\n",
      "Model: 21\n",
      "Model: 22\n",
      "Model: 23\n",
      "Model: 24\n",
      "Model: 25\n",
      "Model: 26\n",
      "Model: 27\n",
      "Model: 28\n",
      "Model: 29\n",
      "Model: 30\n",
      "Model: 31\n",
      "Model: 32\n",
      "Model: 33\n",
      "Model: 34\n",
      "Model: 35\n",
      "Model: 36\n",
      "Model: 37\n",
      "Model: 38\n",
      "Model: 39\n"
     ]
    }
   ],
   "source": [
    "result = np.zeros((x_test.shape[0], 2))\n",
    "\n",
    "for i in range(total_num_model):\n",
    "    print('Model: '+str(i))\n",
    "    model = model_list[i]\n",
    "    result += model.predict(x_test)\n",
    "        \n",
    "result = result/total_num_model # 평균값을 이용해서 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_ensemble = []\n",
    "y_real = []\n",
    "for i in range(len(result)):\n",
    "    if result[i,1] > 0.5:\n",
    "        y_pred_ensemble.append(1)\n",
    "    else:\n",
    "        y_pred_ensemble.append(0)\n",
    "    \n",
    "    if y_test[i,1] > 0.5:\n",
    "        y_real.append(1)\n",
    "    else:\n",
    "        y_real.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.854320987654321"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 앙상블 테스트 결과\n",
    "sum(np.equal(np.array(y_pred_ensemble), np.array(y_real)))/len(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
