{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) Word embedding model import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thehb\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\smart_open\\smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "embedding_model = Word2Vec.load(r'C:\\Users\\thehb\\Code\\sentenceExtraction_with_Article\\embedding_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = embedding_model.wv\n",
    "vocabs = word_vectors.vocab.keys()\n",
    "word_vectors_list = [word_vectors[v] for v in vocabs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) Train data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv파일 불러옴\n",
    "import csv\n",
    "import re\n",
    "\n",
    "raw = []\n",
    "label_damage = []\n",
    "f = open(r'C:\\Users\\thehb\\Code\\sentenceExtraction_with_Article\\sentences_damage2.csv', 'r', encoding='ansi')\n",
    "rdr = csv.reader(f)\n",
    "for line in rdr:\n",
    "    raw.append(line[0])\n",
    "    label_damage.append(line[1])\n",
    "f.close()  \n",
    "\n",
    "# 빈 text 제거 & 특수문자 제거\n",
    "sents = []\n",
    "for i in range(len(raw)):\n",
    "    if(raw[i] is not ' '):\n",
    "        newsent = re.sub('[-=+,#/\\?:^$@*\\\"~&%!<>()\"]', '', raw[i])\n",
    "        sents.append(newsent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 품사 태깅(주어진 텍스트를 형태소 단위로 나누고 나눠진 형태소를 해당하는 품사와 함께 리스트화)\n",
    "# stemming(단어들을 원형으로 포현), normalization(표현 방법이 다른 단어들을 통합시켜 같은 단어로 만듬)\n",
    "import json\n",
    "import os\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "okt = Okt()\n",
    "\n",
    "def tokenize(doc): # 형태소 분석기인 Okt를 이용해서 품사를 태깅시킴\n",
    "    # norm은 정규화, stem은 근어로 표시하기를 나타냄\n",
    "    return ['/'.join(t) for t in okt.pos(doc, norm=True, stem=True)]\n",
    "\n",
    "os.chdir(r'C:\\Users\\thehb\\Code\\sentenceExtraction_with_Article') # 경로 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3) 문장을 vector로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_sents = [] # word_vectors의 key가 '단어/품사'의 형식으로 되어 있기 때문에, 같은 형식으로 만들기 위해 각 문장별로 tokenize를 적용함.\n",
    "for sent in sents:\n",
    "    token_sents.append(tokenize(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_sents = []\n",
    "\n",
    "for token_sent in token_sents:\n",
    "    temp_sent = []\n",
    "    for token_word in token_sent:\n",
    "        # '만조/Noun' 같은 단어가 word_vectors 안에 없는 경우가 있어서\n",
    "        # 이런 경우에는 단어를 추가하지 않고, 지나가도록 만듦.\n",
    "        try:\n",
    "            temp_sent.append(word_vectors.get_vector(token_word))\n",
    "        except KeyError:# key가 존재하지 않을 때\n",
    "            pass # 그냥 지나감        \n",
    "    vec_sents.append(temp_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델링"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) 문장 속 단어들의 Vector의 평균값을 이용해서 문장 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "sentences = np.asarray(vec_sents)\n",
    "mean_sents = []\n",
    "for i in range(len(sentences)):\n",
    "    mean_sents.append(np.mean(sentences[i], axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.42276422764227645"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.array(label_damage) == '1')/len(label_damage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross-val-score \n",
      "[0.82894737 0.83783784 0.80821918 0.89041096 0.84931507]\n",
      "cross-val-score.mean \n",
      "0.843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thehb\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:652: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), Warning)\n",
      "C:\\Users\\thehb\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\thehb\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\thehb\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "C:\\Users\\thehb\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\thehb\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "C:\\Users\\thehb\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\thehb\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "C:\\Users\\thehb\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\thehb\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "scores = cross_val_score(logreg, mean_sents, label_damage, cv=5) \n",
    "print('cross-val-score \\n{}'.format(scores))\n",
    "print('cross-val-score.mean \\n{:.3f}'.format(scores.mean()))\n",
    "# 전체에서 0인 것 57%, 1인 것 43%인데 결과가 84%면 괜찮은 듯.. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) CNN을 이용해서 문장 분류"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
